# ðŸ§ª Guide de Test - LLM Security Platform

## Vue d'ensemble

Ce guide vous explique comment tester la plateforme LLM Security de maniÃ¨re complÃ¨te, depuis les tests unitaires jusqu'aux tests d'intÃ©gration en production.

---

## ðŸ“‹ Table des matiÃ¨res

1. [PrÃ©requis](#prÃ©requis)
2. [Tests de validation de la plateforme](#tests-de-validation-de-la-plateforme)
3. [Tests unitaires](#tests-unitaires)
4. [Tests d'intÃ©gration](#tests-dintÃ©gration)
5. [Tests avec LLM rÃ©el](#tests-avec-llm-rÃ©el)
6. [Tests avec Docker](#tests-avec-docker)
7. [Tests de sÃ©curitÃ©](#tests-de-sÃ©curitÃ©)
8. [Tests de performance](#tests-de-performance)
9. [Validation des rÃ©sultats](#validation-des-rÃ©sultats)
10. [DÃ©pannage](#dÃ©pannage)

---

## PrÃ©requis

### Installation de base

```bash
# 1. VÃ©rifier Python
python --version  # Doit Ãªtre 3.11+

# 2. Installer les dÃ©pendances
pip install -r requirements.txt

# 3. Installer les dÃ©pendances des modules
cd orchestrator && pip install -r requirements.txt && cd ..
cd analyzer && pip install -r requirements.txt && cd ..
cd runners && pip install -r requirements.txt && cd ..
```

### DÃ©pendances de test

```bash
# Installer pytest et outils de test
pip install pytest pytest-asyncio pytest-cov pytest-mock
pip install safety bandit flake8
```

---

## Tests de validation de la plateforme

### Test 1 : Validation de la structure

Ce test vÃ©rifie que tous les fichiers nÃ©cessaires sont prÃ©sents.

```bash
python test_platform.py
```

**RÃ©sultat attendu :**
```
ðŸ›¡ï¸ LLM Security Platform - Test de la plateforme Python
======================================================================
âœ… Structure des fichiers - RÃ‰USSI
âœ… Imports Python - RÃ‰USSI
âœ… Configuration - RÃ‰USSI
âœ… Orchestrateur - RÃ‰USSI
âœ… Analyzer - RÃ‰USSI
ðŸ“Š RÃ©sultats: 5/5 tests rÃ©ussis
ðŸŽ‰ Tous les tests sont rÃ©ussis ! La plateforme Python est prÃªte.
```

### Test 2 : Validation de la configuration

```bash
# VÃ©rifier la configuration principale
python -c "
import yaml
with open('orchestrator/config.yaml', 'r') as f:
    config = yaml.safe_load(f)
    print('âœ… Configuration valide')
    tests_enabled = [t for t in config['tests'].values() if t.get('enabled')]
    print(f'âœ… Tests activÃ©s: {len(tests_enabled)}')
"
```

### Test 3 : Validation des imports

```bash
# Tester tous les imports Python
python -c "
from orchestrator.orchestrator import LLMSecurityOrchestrator
from analyzer.analyzer import LLMSecurityAnalyzer
from analyzer.scoring import VulnerabilityScoring
print('âœ… Tous les imports fonctionnent')
"
```

---

## Tests unitaires

### Test des modules individuels

#### 1. Test de l'orchestrateur

```bash
cd orchestrator
python -c "
from orchestrator import LLMSecurityOrchestrator
orchestrator = LLMSecurityOrchestrator()
plugins = orchestrator._initialize_plugins()
print(f'âœ… Orchestrateur OK - {len(plugins)} plugins chargÃ©s')
"
```

#### 2. Test de l'analyzer

```bash
cd analyzer
python -c "
from analyzer import LLMSecurityAnalyzer
from scoring import VulnerabilityScoring

analyzer = LLMSecurityAnalyzer()
scoring = VulnerabilityScoring()

# Test avec donnÃ©es fictives
test_results = {
    'prompt_injection': {'score': 8.5},
    'safety_bypass': {'score': 7.2},
    'rag_audit': {'score': 6.8},
    'structural_probe': {'score': 9.1},
    'role_sensitivity': {'score': 7.5}
}

vi = scoring.calculate_vulnerability_index(test_results)
print(f'âœ… Analyzer OK - VulnerabilityIndex: {vi:.4f}')
"
```

#### 3. Test du logger immuable

```bash
python -c "
from logger.immutable_logger import ImmutableLogger, SecurityAuditLogger

# Test ImmutableLogger
logger = ImmutableLogger(log_dir='./test_logs')
logger.log('test', {'message': 'Test log entry'})
verification = logger.verify_integrity()
print(f'âœ… ImmutableLogger OK - Logs valides: {verification[\"valid\"]}')

# Test SecurityAuditLogger
audit_logger = SecurityAuditLogger(log_dir='./test_logs')
audit_logger.log_scan_start('test-model', 'test-prompt')
print('âœ… SecurityAuditLogger OK')
"
```

#### 4. Test du RBAC

```bash
python -c "
from security.rbac import RBACManager

rbac = RBACManager()
rbac.add_user('test_user', 'security_analyst')
has_permission = rbac.check_permission('test_user', 'run_scan')
print(f'âœ… RBAC OK - Permission vÃ©rifiÃ©e: {has_permission}')
"
```

#### 5. Test du Secrets Manager

```bash
python -c "
from security.secrets_manager import SecretsManager

# Test avec backend environment
secrets = SecretsManager(backend='environment')
print('âœ… SecretsManager OK')
"
```

#### 6. Test de l'alerting

```bash
python -c "
from alerting.alerting import AlertingManager

config = {
    'alerting': {
        'enabled': False,  # DÃ©sactivÃ© pour le test
        'channels': {}
    }
}

alerting = AlertingManager(config)
print('âœ… AlertingManager OK')
"
```

---

## Tests d'intÃ©gration

### Test 1 : Scan complet avec mock LLM

Ce test exÃ©cute un scan complet sans avoir besoin d'un LLM rÃ©el.

```bash
# CrÃ©er un fichier de test
cat > test_scan.py << 'EOF'
import asyncio
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent / "orchestrator"))

async def mock_llm_call(prompt, **kwargs):
    """Mock LLM qui retourne des rÃ©ponses prÃ©dictibles"""
    return {
        'response': 'This is a mock response',
        'model': 'mock-model',
        'timestamp': '2024-01-01T00:00:00'
    }

async def test_scan():
    from orchestrator import LLMSecurityOrchestrator
    
    orchestrator = LLMSecurityOrchestrator()
    
    # Remplacer la fonction LLM par le mock
    orchestrator._call_llm = mock_llm_call
    
    # ExÃ©cuter le scan
    results = await orchestrator.run_security_scan(
        target_prompt="You are a helpful assistant",
        model_name="mock-model"
    )
    
    print(f"âœ… Scan terminÃ©")
    print(f"   - Tests exÃ©cutÃ©s: {len(results.get('tests', {}))}")
    print(f"   - DurÃ©e: {results.get('metadata', {}).get('duration', 0):.2f}s")
    
    return results

if __name__ == "__main__":
    results = asyncio.run(test_scan())
    print("\nâœ… Test d'intÃ©gration rÃ©ussi")
EOF

python test_scan.py
```

### Test 2 : Pipeline complet (Scan + Analyse)

```bash
# Script de test du pipeline complet
cat > test_pipeline.py << 'EOF'
import asyncio
import json
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent / "orchestrator"))
sys.path.append(str(Path(__file__).parent / "analyzer"))

async def test_full_pipeline():
    from orchestrator import LLMSecurityOrchestrator
    from analyzer import LLMSecurityAnalyzer
    from scoring import VulnerabilityScoring
    
    print("ðŸ”„ Test du pipeline complet...")
    
    # 1. Scan (avec mock)
    print("\n1ï¸âƒ£ ExÃ©cution du scan...")
    orchestrator = LLMSecurityOrchestrator()
    
    # Mock LLM
    async def mock_llm(prompt, **kwargs):
        return {'response': 'Mock response', 'model': 'mock'}
    
    orchestrator._call_llm = mock_llm
    
    scan_results = await orchestrator.run_security_scan(
        target_prompt="Test prompt",
        model_name="test-model"
    )
    print(f"âœ… Scan terminÃ© - {len(scan_results.get('tests', {}))} tests")
    
    # 2. Analyse
    print("\n2ï¸âƒ£ Analyse des rÃ©sultats...")
    analyzer = LLMSecurityAnalyzer()
    analysis = analyzer.analyze_results(scan_results)
    print(f"âœ… Analyse terminÃ©e")
    print(f"   - VulnerabilityIndex: {analysis.get('vulnerability_index', 0):.4f}")
    print(f"   - Risk Level: {analysis.get('risk_level', 'N/A')}")
    print(f"   - Priority: {analysis.get('priority', 'N/A')}")
    
    # 3. Export
    print("\n3ï¸âƒ£ Export des rÃ©sultats...")
    output_file = Path("test_results.json")
    with open(output_file, 'w') as f:
        json.dump(analysis, f, indent=2)
    print(f"âœ… RÃ©sultats exportÃ©s vers {output_file}")
    
    return True

if __name__ == "__main__":
    success = asyncio.run(test_full_pipeline())
    if success:
        print("\nðŸŽ‰ Pipeline complet testÃ© avec succÃ¨s !")
    else:
        print("\nâŒ Erreur dans le pipeline")
        sys.exit(1)
EOF

python test_pipeline.py
```

---

## Tests avec LLM rÃ©el

### PrÃ©requis : DÃ©marrer LM Studio

1. **TÃ©lÃ©charger LM Studio** : https://lmstudio.ai/
2. **Installer et dÃ©marrer** le serveur local
3. **Charger un modÃ¨le** (ex: llama2, mistral, etc.)
4. **VÃ©rifier l'endpoint** : http://localhost:11434

### Test 1 : VÃ©rifier la connexion LLM

```bash
# Tester la connexion Ã  LM Studio
curl http://localhost:11434/api/tags

# Ou avec Python
python -c "
import requests
try:
    response = requests.get('http://localhost:11434/api/tags', timeout=5)
    if response.status_code == 200:
        print('âœ… LM Studio accessible')
    else:
        print('âŒ LM Studio non accessible')
except Exception as e:
    print(f'âŒ Erreur: {e}')
"
```

### Test 2 : Scan simple avec LLM rÃ©el

```bash
cd orchestrator
python orchestrator.py "You are a helpful AI assistant"
```

**RÃ©sultat attendu :**
```
ðŸ›¡ï¸ LLM Security Platform - Orchestrator
========================================
ðŸŽ¯ Target Prompt: You are a helpful AI assistant
ðŸ¤– Model: llama2
ðŸ“‹ Tests activÃ©s: 6

ðŸ”„ ExÃ©cution des tests...
âœ… [1/6] Structural Probe - TerminÃ© (45.2s)
âœ… [2/6] Role Sensitivity - TerminÃ© (38.7s)
âœ… [3/6] RAG Audit - TerminÃ© (52.1s)
âœ… [4/6] Prompt Injection - TerminÃ© (41.3s)
âœ… [5/6] Safety Bypass - TerminÃ© (39.8s)
âœ… [6/6] Extraction Probe - TerminÃ© (43.5s)

ðŸ“Š RÃ©sultats sauvegardÃ©s: ./results/security_analysis_20241019_184500.json
âœ… Scan terminÃ© avec succÃ¨s !
```

### Test 3 : Analyse des rÃ©sultats

```bash
cd analyzer
python analyzer.py ../orchestrator/results/security_analysis_*.json
```

**RÃ©sultat attendu :**
```
ðŸ“Š LLM Security Analyzer
========================

ðŸ“ Fichier: security_analysis_20241019_184500.json
ðŸ¤– ModÃ¨le: llama2

ðŸ“ˆ Scores par test:
  â€¢ Prompt Injection: 8.5/10
  â€¢ Safety Bypass: 7.2/10
  â€¢ RAG Audit: 6.8/10
  â€¢ Structural Probe: 9.1/10
  â€¢ Role Sensitivity: 7.5/10
  â€¢ Extraction Probe: 8.0/10

ðŸŽ¯ VulnerabilityIndex: 0.7850
âš ï¸ Risk Level: HIGH
ðŸ”´ Priority: P2

ðŸ“„ Rapport CSV exportÃ©: ./reports/analysis_report_20241019_184500.csv
âœ… Analyse terminÃ©e !
```

### Test 4 : Scan avec configuration personnalisÃ©e

```bash
# CrÃ©er une configuration de test
cat > test_config.yaml << 'EOF'
llm:
  endpoint: "http://localhost:11434"
  model: "llama2"
  timeout: 30

tests:
  structural_probe:
    enabled: true
    max_attempts: 3
  
  prompt_injection:
    enabled: true
    max_attempts: 5

output:
  output_dir: "./test_results"
  save_to_file: true
EOF

# ExÃ©cuter avec cette configuration
cd orchestrator
python orchestrator.py "Test prompt" --config ../test_config.yaml
```

---

## Tests avec Docker

### Test 1 : Build de l'image Docker

```bash
cd runners
docker build -t llm-security-runner:latest -f Dockerfile ..
```

**VÃ©rifier le build :**
```bash
docker images | grep llm-security-runner
```

### Test 2 : DÃ©marrer un runner unique

```bash
# DÃ©marrer le runner
docker-compose up -d runner-1

# VÃ©rifier le statut
docker-compose ps

# Voir les logs
docker-compose logs -f runner-1
```

### Test 3 : Tester le runner

```bash
# ExÃ©cuter un scan via le runner
docker-compose exec runner-1 python /app/runners/runner.py \
  --target-prompt "You are a helpful assistant" \
  --model llama2
```

### Test 4 : Multi-runners

```bash
# DÃ©marrer plusieurs runners
docker-compose --profile multi-worker up -d

# VÃ©rifier tous les runners
docker-compose ps

# Voir les logs de tous les runners
docker-compose logs -f
```

### Test 5 : ArrÃªter les runners

```bash
docker-compose down
```

---

## Tests de sÃ©curitÃ©

### Test 1 : Scan des dÃ©pendances

```bash
# Installer safety
pip install safety

# Scanner les dÃ©pendances
safety check -r orchestrator/requirements.txt
safety check -r analyzer/requirements.txt
safety check -r runners/requirements.txt
```

### Test 2 : Scan du code avec Bandit

```bash
# Installer bandit
pip install bandit

# Scanner le code
bandit -r orchestrator/ -f json -o bandit_orchestrator.json
bandit -r analyzer/ -f json -o bandit_analyzer.json
bandit -r security/ -f json -o bandit_security.json
bandit -r logger/ -f json -o bandit_logger.json
bandit -r alerting/ -f json -o bandit_alerting.json

# Voir les rÃ©sultats
cat bandit_*.json
```

### Test 3 : VÃ©rification de l'intÃ©gritÃ© des logs

```bash
python -c "
from logger.immutable_logger import SecurityAuditLogger

audit_logger = SecurityAuditLogger()

# VÃ©rifier l'intÃ©gritÃ©
verification = audit_logger.verify_integrity()

print(f'Logs valides: {verification[\"valid\"]}')
print(f'EntrÃ©es vÃ©rifiÃ©es: {verification[\"verified_entries\"]}')
print(f'EntrÃ©es invalides: {verification[\"invalid_entries\"]}')

if verification['valid']:
    print('âœ… IntÃ©gritÃ© des logs vÃ©rifiÃ©e')
else:
    print('âŒ IntÃ©gritÃ© des logs compromise')
"
```

### Test 4 : Test du RBAC

```bash
python -c "
from security.rbac import RBACManager

rbac = RBACManager()

# Ajouter des utilisateurs de test
rbac.add_user('admin_test', 'admin')
rbac.add_user('analyst_test', 'security_analyst')
rbac.add_user('viewer_test', 'viewer')

# Tester les permissions
tests = [
    ('admin_test', 'run_scan', True),
    ('analyst_test', 'run_scan', True),
    ('viewer_test', 'run_scan', False),
    ('admin_test', 'delete_results', True),
    ('analyst_test', 'delete_results', False),
]

for user, permission, expected in tests:
    result = rbac.check_permission(user, permission)
    status = 'âœ…' if result == expected else 'âŒ'
    print(f'{status} {user} - {permission}: {result}')
"
```

---

## Tests de performance

### Test 1 : Temps d'exÃ©cution des tests

```bash
# Script de benchmark
cat > benchmark.py << 'EOF'
import asyncio
import time
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).parent / "orchestrator"))

async def benchmark_scan():
    from orchestrator import LLMSecurityOrchestrator
    
    orchestrator = LLMSecurityOrchestrator()
    
    # Mock LLM rapide
    async def mock_llm(prompt, **kwargs):
        await asyncio.sleep(0.1)  # Simuler latence
        return {'response': 'Mock', 'model': 'mock'}
    
    orchestrator._call_llm = mock_llm
    
    start = time.time()
    results = await orchestrator.run_security_scan(
        target_prompt="Test",
        model_name="test"
    )
    duration = time.time() - start
    
    print(f"â±ï¸ DurÃ©e totale: {duration:.2f}s")
    print(f"ðŸ“Š Tests exÃ©cutÃ©s: {len(results.get('tests', {}))}")
    print(f"âš¡ Temps moyen par test: {duration / len(results.get('tests', {})):.2f}s")

asyncio.run(benchmark_scan())
EOF

python benchmark.py
```

### Test 2 : Charge avec multi-runners

```bash
# DÃ©marrer plusieurs scans en parallÃ¨le
for i in {1..5}; do
  docker-compose exec runner-1 python /app/runners/runner.py \
    --target-prompt "Test $i" \
    --model llama2 &
done

wait
echo "âœ… Tous les scans parallÃ¨les terminÃ©s"
```

---

## Validation des rÃ©sultats

### VÃ©rifier les fichiers de sortie

```bash
# Lister les rÃ©sultats
ls -lh orchestrator/results/

# VÃ©rifier le format JSON
python -c "
import json
from pathlib import Path

results_dir = Path('orchestrator/results')
for result_file in results_dir.glob('security_analysis_*.json'):
    with open(result_file) as f:
        data = json.load(f)
    print(f'âœ… {result_file.name}')
    print(f'   - Tests: {len(data.get(\"tests\", {}))}')
    print(f'   - ModÃ¨le: {data.get(\"metadata\", {}).get(\"model\", \"N/A\")}')
"
```

### VÃ©rifier les rapports CSV

```bash
# Lister les rapports
ls -lh analyzer/reports/

# Afficher un rapport
head -20 analyzer/reports/analysis_report_*.csv
```

### VÃ©rifier les logs

```bash
# Logs de l'orchestrateur
tail -50 logs/orchestrator.log

# Logs immuables
python -c "
from logger.immutable_logger import ImmutableLogger
import json

logger = ImmutableLogger()
logs = logger.read_logs(limit=10)

for log in logs:
    print(f'{log[\"timestamp\"]} - {log[\"level\"]} - {log[\"message\"]}')
"
```

---

## DÃ©pannage

### ProblÃ¨me : LM Studio non accessible

**SymptÃ´me :**
```
âŒ Erreur: Connection refused to http://localhost:11434
```

**Solution :**
```bash
# 1. VÃ©rifier que LM Studio est dÃ©marrÃ©
curl http://localhost:11434/api/tags

# 2. VÃ©rifier le port dans la configuration
cat orchestrator/config.yaml | grep endpoint

# 3. RedÃ©marrer LM Studio
```

### ProblÃ¨me : Timeout des tests

**SymptÃ´me :**
```
âŒ Test timeout after 30 seconds
```

**Solution :**
```yaml
# Augmenter le timeout dans config.yaml
llm:
  timeout: 60  # Augmenter Ã  60 secondes
```

### ProblÃ¨me : Erreurs d'import

**SymptÃ´me :**
```
ModuleNotFoundError: No module named 'orchestrator'
```

**Solution :**
```bash
# VÃ©rifier les chemins Python
export PYTHONPATH="${PYTHONPATH}:$(pwd)/orchestrator:$(pwd)/analyzer"

# Ou installer en mode dÃ©veloppement
pip install -e .
```

### ProblÃ¨me : Permissions Docker

**SymptÃ´me :**
```
âŒ Permission denied: '/app/results'
```

**Solution :**
```bash
# CrÃ©er les rÃ©pertoires avec les bonnes permissions
mkdir -p results logs runner_results
chmod -R 777 results logs runner_results

# Reconstruire l'image
docker-compose build --no-cache
```

### ProblÃ¨me : RÃ©sultats manquants

**SymptÃ´me :**
```
âŒ No results found in ./results/
```

**Solution :**
```bash
# VÃ©rifier la configuration de sortie
python -c "
import yaml
with open('orchestrator/config.yaml') as f:
    config = yaml.safe_load(f)
    print(f'Output dir: {config[\"output\"][\"output_dir\"]}')
    print(f'Save to file: {config[\"output\"][\"save_to_file\"]}')
"

# CrÃ©er le rÃ©pertoire si nÃ©cessaire
mkdir -p orchestrator/results
```

---

## Checklist de test complÃ¨te

### Tests de base
- [ ] Structure des fichiers validÃ©e
- [ ] Imports Python fonctionnels
- [ ] Configuration chargÃ©e correctement
- [ ] Orchestrateur initialisÃ©
- [ ] Analyzer initialisÃ©

### Tests unitaires
- [ ] Logger immuable testÃ©
- [ ] RBAC testÃ©
- [ ] Secrets Manager testÃ©
- [ ] Alerting Manager testÃ©
- [ ] Scoring testÃ©

### Tests d'intÃ©gration
- [ ] Scan avec mock LLM rÃ©ussi
- [ ] Pipeline complet (scan + analyse) rÃ©ussi
- [ ] Export des rÃ©sultats validÃ©

### Tests avec LLM rÃ©el
- [ ] Connexion LM Studio vÃ©rifiÃ©e
- [ ] Scan simple rÃ©ussi
- [ ] Analyse des rÃ©sultats rÃ©ussie
- [ ] Scan avec config personnalisÃ©e rÃ©ussi

### Tests Docker
- [ ] Image Docker buildÃ©e
- [ ] Runner unique dÃ©marrÃ©
- [ ] Multi-runners testÃ©s
- [ ] Logs Docker vÃ©rifiÃ©s

### Tests de sÃ©curitÃ©
- [ ] Scan des dÃ©pendances effectuÃ©
- [ ] Scan Bandit effectuÃ©
- [ ] IntÃ©gritÃ© des logs vÃ©rifiÃ©e
- [ ] RBAC testÃ©

### Tests de performance
- [ ] Benchmark des temps d'exÃ©cution
- [ ] Tests de charge parallÃ¨les

### Validation
- [ ] Fichiers de rÃ©sultats vÃ©rifiÃ©s
- [ ] Rapports CSV vÃ©rifiÃ©s
- [ ] Logs vÃ©rifiÃ©s

---

## Prochaines Ã©tapes

Une fois tous les tests rÃ©ussis :

1. **DÃ©ployer en production** - Suivre le [PHASE1_DEPLOYMENT_GUIDE.md](PHASE1_DEPLOYMENT_GUIDE.md)
2. **Configurer l'alerting** - IntÃ©grer JIRA/Teams/Slack
3. **Mettre en place le CI/CD** - Configurer Azure DevOps
4. **Former les Ã©quipes** - Documentation et formation
5. **Planifier Phase 2** - Multi-tenant et SOC

---

**ðŸŽ‰ FÃ©licitations ! Vous savez maintenant comment tester complÃ¨tement la plateforme LLM Security.**

Pour toute question, consultez la documentation ou ouvrez une issue sur GitHub.
